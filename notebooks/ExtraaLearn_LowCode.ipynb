{"cells":[{"cell_type":"markdown","id":"dense-medicaid","metadata":{"id":"dense-medicaid"},"source":["# ExtraaLearn Project\n","***Marks: 60***\n","\n","## Context\n","\n","The EdTech industry has been surging in the past decade immensely, and according to a forecast, the Online Education market would be worth $286.62bn by 2023 with a compound annual growth rate (CAGR) of 10.26% from 2018 to 2023. The modern era of online education has enforced a lot in its growth and expansion beyond any limit. Due to having many dominant features like ease of information sharing, personalized learning experience, transparency of assessment, etc, it is now preferable to traditional education.\n","\n","In the present scenario due to the Covid-19, the online education sector has witnessed rapid growth and is attracting a lot of new customers. Due to this rapid growth, many new companies have emerged in this industry. With the availability and ease of use of digital marketing resources, companies can reach out to a wider audience with their offerings. The customers who show interest in these offerings are termed as leads. There are various sources of obtaining leads for Edtech companies, like\n","\n","* The customer interacts with the marketing front on social media or other online platforms.\n","* The customer browses the website/app and downloads the brochure\n","* The customer connects through emails for more information.\n","\n","The company then nurtures these leads and tries to convert them to paid customers. For this, the representative from the organization connects with the lead on call or through email to share further details.\n","\n","## Objective\n","\n","ExtraaLearn is an initial stage startup that offers programs on cutting-edge technologies to students and professionals to help them upskill/reskill. With a large number of leads being generated regularly, one of the issues faced by ExtraaLearn is to identify which of the leads are more likely to convert so that they can allocate resources accordingly. You, as a data scientist at ExtraaLearn, have been provided the leads data to:\n","* Analyze and build an ML model to help identify which leads are more likely to convert to paid customers,\n","* Find the factors driving the lead conversion process\n","* Create a profile of the leads which are likely to convert\n","\n","\n","## Data Description\n","\n","The data contains the different attributes of leads and their interaction details with ExtraaLearn. The detailed data dictionary is given below.\n","\n","\n","**Data Dictionary**\n","* ID: ID of the lead\n","* age: Age of the lead\n","* current_occupation: Current occupation of the lead. Values include 'Professional','Unemployed',and 'Student'\n","* first_interaction: How did the lead first interact with ExtraaLearn. Values include 'Website', 'Mobile App'\n","* profile_completed: What percentage of the profile has been filled by the lead on the website/mobile app. Values include Low - (0-50%), Medium - (50-75%), High (75-100%)\n","* website_visits: How many times has a lead visited the website\n","* time_spent_on_website: Total time spent on the website\n","* page_views_per_visit: Average number of pages on the website viewed during the visits.\n","* last_activity: Last interaction between the lead and ExtraaLearn.\n","    * Email Activity: Seeking for details about the program through email, Representative shared information with a lead like a brochure of program, etc\n","    * Phone Activity: Had a Phone Conversation with a representative, Had conversation over SMS with a representative, etc\n","    * Website Activity: Interacted on live chat with a representative, Updated profile on the website, etc\n","\n","* print_media_type1: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Newspaper.\n","* print_media_type2: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Magazine.\n","* digital_media: Flag indicating whether the lead had seen the ad of ExtraaLearn on the digital platforms.\n","* educational_channels: Flag indicating whether the lead had heard about ExtraaLearn in the education channels like online forums, discussion threads, educational websites, etc.\n","* referral: Flag indicating whether the lead had heard about ExtraaLearn through reference.\n","* status: Flag indicating whether the lead was converted to a paid customer or not."]},{"cell_type":"markdown","id":"d0ef2b3c","metadata":{"id":"d0ef2b3c"},"source":["### **Please read the instructions carefully before starting the project.**\n","This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n","* Blanks '_______' are provided in the notebook that\n","needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n","* Identify the task to be performed correctly, and only then proceed to write the required code.\n","* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n","* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n","* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3842e4e"},"outputs":[],"source":["# Installing the libraries with the specified version.\n","!pip install pandas==1.5.3 numpy==1.25.2 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 statsmodels==0.14.1 -q --user"],"id":"d3842e4e"},{"cell_type":"markdown","metadata":{"id":"9ac4dfda"},"source":["**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"],"id":"9ac4dfda"},{"cell_type":"code","source":["# Libraries to help with reading and manipulating data\n","import pandas as pd\n","import numpy as np\n","\n","# libaries to help with data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Removes the limit for the number of displayed columns\n","pd.set_option(\"display.max_columns\", None)\n","# Sets the limit for the number of displayed rows\n","pd.set_option(\"display.max_rows\", 200)\n","# setting the precision of floating numbers to 5 decimal points\n","pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n","\n","# Library to split data\n","from sklearn.model_selection import train_test_split\n","\n","# To build model for prediction\n","from sklearn.linear_model import LogisticRegression\n","import statsmodels.stats.api as sms\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","import statsmodels.api as sm\n","from statsmodels.tools.tools import add_constant\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","\n","# To tune different models\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","# To get diferent metric scores\n","from sklearn.metrics import (\n","    f1_score,\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    confusion_matrix,\n","    roc_auc_score,\n","    precision_recall_curve,\n","    roc_curve,\n","    make_scorer,\n",")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from statsmodels.tools.sm_exceptions import ConvergenceWarning\n","warnings.simplefilter(\"ignore\", ConvergenceWarning)"],"metadata":{"id":"e0Kh2lF72uKX"},"execution_count":null,"outputs":[],"id":"e0Kh2lF72uKX"},{"cell_type":"markdown","id":"fantastic-rebel","metadata":{"id":"fantastic-rebel"},"source":["## Import Dataset"]},{"cell_type":"code","execution_count":null,"id":"precious-leonard","metadata":{"id":"precious-leonard"},"outputs":[],"source":["learn = pd.read_csv(\"______\") ##  Complete the code to read the data"]},{"cell_type":"code","execution_count":null,"id":"geographic-gender","metadata":{"id":"geographic-gender"},"outputs":[],"source":["# copying data to another variable to avoid any changes to original data\n","data = learn.copy()"]},{"cell_type":"markdown","id":"convinced-blackberry","metadata":{"id":"convinced-blackberry"},"source":["### View the first and last 5 rows of the dataset"]},{"cell_type":"code","execution_count":null,"id":"tested-adjustment","metadata":{"id":"tested-adjustment"},"outputs":[],"source":["data.\"_______\" ##  Complete the code to view top 5 rows of the data"]},{"cell_type":"code","execution_count":null,"id":"demonstrated-charger","metadata":{"id":"demonstrated-charger"},"outputs":[],"source":["data.\"_______\" ##  Complete the code to view last 5 rows of the data"]},{"cell_type":"markdown","id":"prepared-clause","metadata":{"id":"prepared-clause"},"source":["### Understand the shape of the dataset"]},{"cell_type":"code","execution_count":null,"id":"likely-scene","metadata":{"id":"likely-scene"},"outputs":[],"source":["data.\"________\" ## Complete the code to get the shape of data"]},{"cell_type":"markdown","id":"creative-warner","metadata":{"id":"creative-warner"},"source":["### Check the data types of the columns for the dataset"]},{"cell_type":"code","execution_count":null,"id":"expanded-technique","metadata":{"id":"expanded-technique"},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"id":"greenhouse-vertical","metadata":{"id":"greenhouse-vertical"},"outputs":[],"source":["# checking for duplicate values\n","data.\"__________\" ## Complete the code to check duplicate entries in the data"]},{"cell_type":"markdown","id":"realistic-mortgage","metadata":{"id":"realistic-mortgage"},"source":["## Exploratory Data Analysis"]},{"cell_type":"markdown","id":"seeing-newman","metadata":{"id":"seeing-newman"},"source":["**Let's check the statistical summary of the data.**"]},{"cell_type":"code","execution_count":null,"id":"backed-solution","metadata":{"id":"backed-solution"},"outputs":[],"source":["data.'_______' ##  Complete the code to print the statistical summary of the data"]},{"cell_type":"code","execution_count":null,"id":"cultural-plaza","metadata":{"id":"cultural-plaza"},"outputs":[],"source":["# Making a list of all catrgorical variables\n","cat_col = list(data.select_dtypes(\"object\").columns)\n","\n","# Printing number of count of each unique value in each column\n","for column in cat_col:\n","    print(data[column].value_counts())\n","    print(\"-\" * 50)"]},{"cell_type":"code","execution_count":null,"id":"victorian-chuck","metadata":{"id":"victorian-chuck"},"outputs":[],"source":["# checking the number of unique values\n","data[\"ID\"].\"_______\" # Complete the code to check the number of unique values"]},{"cell_type":"code","execution_count":null,"id":"comprehensive-heavy","metadata":{"id":"comprehensive-heavy"},"outputs":[],"source":["data.\"_________\"([\"ID\"], axis=1, inplace=True) # Complete the code to drop \"ID\" column from data"]},{"cell_type":"markdown","id":"arbitrary-intelligence","metadata":{"id":"arbitrary-intelligence"},"source":["### Univariate Analysis"]},{"cell_type":"code","execution_count":null,"id":"several-cheese","metadata":{"id":"several-cheese"},"outputs":[],"source":["# function to plot a boxplot and a histogram along the same scale.\n","\n","\n","def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n","    \"\"\"\n","    Boxplot and histogram combined\n","\n","    data: dataframe\n","    feature: dataframe column\n","    figsize: size of figure (default (12,7))\n","    kde: whether to the show density curve (default False)\n","    bins: number of bins for histogram (default None)\n","    \"\"\"\n","    f2, (ax_box2, ax_hist2) = plt.subplots(\n","        nrows=2,  # Number of rows of the subplot grid= 2\n","        sharex=True,  # x-axis will be shared among all subplots\n","        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n","        figsize=figsize,\n","    )  # creating the 2 subplots\n","    sns.boxplot(\n","        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n","    )  # boxplot will be created and a star will indicate the mean value of the column\n","    sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n","    ) if bins else sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2\n","    )  # For histogram\n","    ax_hist2.axvline(\n","        data[feature].mean(), color=\"green\", linestyle=\"--\"\n","    )  # Add mean to the histogram\n","    ax_hist2.axvline(\n","        data[feature].median(), color=\"black\", linestyle=\"-\"\n","    )  # Add median to the histogram"]},{"cell_type":"markdown","id":"western-elevation","metadata":{"id":"western-elevation"},"source":["### Observations on age"]},{"cell_type":"code","execution_count":null,"id":"italian-imagination","metadata":{"id":"italian-imagination"},"outputs":[],"source":["histogram_boxplot(data, \"age\")"]},{"cell_type":"markdown","id":"intimate-hearing","metadata":{"id":"intimate-hearing"},"source":["### Observations on website_visits"]},{"cell_type":"code","execution_count":null,"id":"liable-guess","metadata":{"id":"liable-guess"},"outputs":[],"source":["histogram_boxplot(\"_________\") # Complete the code to plot a histogram_boxplot for website_visits\n"]},{"cell_type":"code","execution_count":null,"id":"southern-organic","metadata":{"id":"southern-organic"},"outputs":[],"source":["# To check how many leads have not visited web-site\n","data[data[\"website_visits\"] == 0].shape"]},{"cell_type":"markdown","id":"studied-arrangement","metadata":{"id":"studied-arrangement"},"source":["### Observations on number of time_spent_on_website"]},{"cell_type":"code","execution_count":null,"id":"molecular-opposition","metadata":{"id":"molecular-opposition"},"outputs":[],"source":["histogram_boxplot(\"_________\") # Complete the code to plot a histogram_boxplot for time_spent_on_website\n"]},{"cell_type":"markdown","id":"rough-contributor","metadata":{"id":"rough-contributor"},"source":["### Observations on number of page_views_per_visit"]},{"cell_type":"code","execution_count":null,"id":"laughing-bridge","metadata":{"id":"laughing-bridge"},"outputs":[],"source":["histogram_boxplot(\"_________\") # Complete the code to plot a histogram_boxplot for page_views_per_visit\n"]},{"cell_type":"code","execution_count":null,"id":"attended-grounds","metadata":{"id":"attended-grounds"},"outputs":[],"source":["# function to create labeled barplots\n","\n","\n","def labeled_barplot(data, feature, perc=False, n=None):\n","    \"\"\"\n","    Barplot with percentage at the top\n","\n","    data: dataframe\n","    feature: dataframe column\n","    perc: whether to display percentages instead of count (default is False)\n","    n: displays the top n category levels (default is None, i.e., display all levels)\n","    \"\"\"\n","\n","    total = len(data[feature])  # length of the column\n","    count = data[feature].nunique()\n","    if n is None:\n","        plt.figure(figsize=(count + 1, 5))\n","    else:\n","        plt.figure(figsize=(n + 1, 5))\n","\n","    plt.xticks(rotation=90, fontsize=15)\n","    ax = sns.countplot(\n","        data=data,\n","        x=feature,\n","        order=data[feature].value_counts().index[:n].sort_values(),\n","    )\n","\n","    for p in ax.patches:\n","        if perc == True:\n","            label = \"{:.1f}%\".format(\n","                100 * p.get_height() / total\n","            )  # percentage of each class of the category\n","        else:\n","            label = p.get_height()  # count of each level of the category\n","\n","        x = p.get_x() + p.get_width() / 2  # width of the plot\n","        y = p.get_height()  # height of the plot\n","\n","        ax.annotate(\n","            label,\n","            (x, y),\n","            ha=\"center\",\n","            va=\"center\",\n","            size=12,\n","            xytext=(0, 5),\n","            textcoords=\"offset points\",\n","        )  # annotate the percentage\n","\n","    plt.show()  # show the plot"]},{"cell_type":"markdown","id":"industrial-implementation","metadata":{"id":"industrial-implementation"},"source":["### Observations on current_occupation"]},{"cell_type":"code","execution_count":null,"id":"israeli-sympathy","metadata":{"id":"israeli-sympathy"},"outputs":[],"source":["labeled_barplot(data, \"current_occupation\", perc=True)"]},{"cell_type":"markdown","id":"celtic-florist","metadata":{"id":"celtic-florist"},"source":["### Observations on number of first_interaction"]},{"cell_type":"code","execution_count":null,"id":"finite-kingston","metadata":{"id":"finite-kingston"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for first_interaction"]},{"cell_type":"markdown","id":"affiliated-accreditation","metadata":{"id":"affiliated-accreditation"},"source":["### Observations on profile_completed"]},{"cell_type":"code","execution_count":null,"id":"great-kitchen","metadata":{"id":"great-kitchen"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for profile_completed"]},{"cell_type":"markdown","id":"thermal-resource","metadata":{"id":"thermal-resource"},"source":["### Observations on last_activity"]},{"cell_type":"code","execution_count":null,"id":"bizarre-serbia","metadata":{"id":"bizarre-serbia","scrolled":false},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for last_activity"]},{"cell_type":"markdown","id":"greenhouse-regression","metadata":{"id":"greenhouse-regression"},"source":["### Observations on print_media_type1"]},{"cell_type":"code","execution_count":null,"id":"handy-talent","metadata":{"id":"handy-talent"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for print_media_type1"]},{"cell_type":"markdown","id":"southeast-avenue","metadata":{"id":"southeast-avenue"},"source":["### Observations on print_media_type2"]},{"cell_type":"code","execution_count":null,"id":"retired-preliminary","metadata":{"id":"retired-preliminary"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for print_media_type2"]},{"cell_type":"markdown","id":"competitive-brass","metadata":{"id":"competitive-brass"},"source":["### Observations on digital_media"]},{"cell_type":"code","execution_count":null,"id":"cordless-assurance","metadata":{"id":"cordless-assurance"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for digital_media"]},{"cell_type":"markdown","id":"thick-coordination","metadata":{"id":"thick-coordination"},"source":["### Observations on educational_channels"]},{"cell_type":"code","execution_count":null,"id":"expensive-deposit","metadata":{"id":"expensive-deposit"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for educational_channels"]},{"cell_type":"markdown","id":"bulgarian-paint","metadata":{"id":"bulgarian-paint"},"source":["### Observations on referral"]},{"cell_type":"code","execution_count":null,"id":"ordinary-foster","metadata":{"id":"ordinary-foster"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for referral"]},{"cell_type":"markdown","id":"behavioral-portfolio","metadata":{"id":"behavioral-portfolio"},"source":["### Observations on status"]},{"cell_type":"code","execution_count":null,"id":"abroad-moldova","metadata":{"id":"abroad-moldova"},"outputs":[],"source":["labeled_barplot(\"____________\") # Complete the code to plot labeled_barplot for status"]},{"cell_type":"markdown","id":"arranged-courtesy","metadata":{"id":"arranged-courtesy"},"source":["### Bivariate Analysis"]},{"cell_type":"code","execution_count":null,"id":"official-wyoming","metadata":{"id":"official-wyoming"},"outputs":[],"source":["cols_list = data.select_dtypes(include=np.number).columns.tolist()\n","\n","plt.figure(figsize=(12, 7))\n","sns.heatmap(\n","    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",")\n","plt.show()"]},{"cell_type":"markdown","id":"upper-glass","metadata":{"id":"upper-glass"},"source":["**Creating functions that will help us with further analysis.**"]},{"cell_type":"code","execution_count":null,"id":"sought-bunny","metadata":{"id":"sought-bunny"},"outputs":[],"source":["### function to plot distributions wrt target\n","\n","\n","def distribution_plot_wrt_target(data, predictor, target):\n","\n","    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n","\n","    target_uniq = data[target].unique()\n","\n","    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n","    sns.histplot(\n","        data=data[data[target] == target_uniq[0]],\n","        x=predictor,\n","        kde=True,\n","        ax=axs[0, 0],\n","        color=\"teal\",\n","        stat=\"density\",\n","    )\n","\n","    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n","    sns.histplot(\n","        data=data[data[target] == target_uniq[1]],\n","        x=predictor,\n","        kde=True,\n","        ax=axs[0, 1],\n","        color=\"orange\",\n","        stat=\"density\",\n","    )\n","\n","    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n","    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0])\n","\n","    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n","    sns.boxplot(\n","        data=data,\n","        x=target,\n","        y=predictor,\n","        ax=axs[1, 1],\n","        showfliers=False,\n","    )\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"quick-progress","metadata":{"id":"quick-progress"},"outputs":[],"source":["def stacked_barplot(data, predictor, target):\n","    \"\"\"\n","    Print the category counts and plot a stacked bar chart\n","\n","    data: dataframe\n","    predictor: independent variable\n","    target: target variable\n","    \"\"\"\n","    count = data[predictor].nunique()\n","    sorter = data[target].value_counts().index[-1]\n","    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n","        by=sorter, ascending=False\n","    )\n","    print(tab1)\n","    print(\"-\" * 120)\n","    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n","        by=sorter, ascending=False\n","    )\n","    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n","    plt.legend(\n","        loc=\"lower left\", frameon=False,\n","    )\n","    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n","    plt.show()"]},{"cell_type":"markdown","id":"abstract-laptop","metadata":{"id":"abstract-laptop"},"source":["**Leads will have different expectations from the outcome of the course and the current occupation may play a key role for them to take the program. Let's analyze it**"]},{"cell_type":"code","execution_count":null,"id":"understood-butterfly","metadata":{"id":"understood-butterfly"},"outputs":[],"source":["stacked_barplot(data, \"current_occupation\", \"status\")"]},{"cell_type":"markdown","id":"pharmaceutical-sequence","metadata":{"id":"pharmaceutical-sequence"},"source":["**Age can be a good factor to differentiate between such leads**"]},{"cell_type":"code","execution_count":null,"id":"laden-siemens","metadata":{"id":"laden-siemens"},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","sns.boxplot(data=data, x=\"current_occupation\", y=\"age\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"structured-aviation","metadata":{"id":"structured-aviation"},"outputs":[],"source":["data.groupby([\"current_occupation\"])[\"age\"].describe()"]},{"cell_type":"markdown","id":"micro-costs","metadata":{"id":"micro-costs"},"source":["**The company's first interaction with leads should be compelling and persuasive. Let's see if the channels of the first interaction have an impact on the conversion of leads**"]},{"cell_type":"code","execution_count":null,"id":"confused-secretariat","metadata":{"id":"confused-secretariat"},"outputs":[],"source":["stacked_barplot(\"_________________\") # Complete the code to plot stacked_barplot for first_interaction and status"]},{"cell_type":"code","execution_count":null,"id":"verbal-columbus","metadata":{"id":"verbal-columbus"},"outputs":[],"source":["distribution_plot_wrt_target(data, \"time_spent_on_website\", \"status\")"]},{"cell_type":"code","execution_count":null,"id":"extraordinary-necklace","metadata":{"id":"extraordinary-necklace"},"outputs":[],"source":["# checking the median value\n","data.groupby([\"status\"])[\"time_spent_on_website\"].median()"]},{"cell_type":"markdown","id":"minus-airline","metadata":{"id":"minus-airline"},"source":["**Let's do a similar analysis for time spent on website and page views per visit.**"]},{"cell_type":"code","execution_count":null,"id":"dedicated-oxford","metadata":{"id":"dedicated-oxford"},"outputs":[],"source":["distribution_plot_wrt_target(\"_____________________\") # Complete the code to plot distribution for website_visits and status"]},{"cell_type":"code","execution_count":null,"id":"complex-request","metadata":{"id":"complex-request"},"outputs":[],"source":["distribution_plot_wrt_target(\"___________________\") # Complete the code to plot distribution for page_views_per_visit and status"]},{"cell_type":"markdown","id":"empirical-destination","metadata":{"id":"empirical-destination"},"source":["**People browsing the website or the mobile app are generally required to create a profile by sharing their personal details before they can access more information. Let's see if the profile completion level has an impact on lead status**"]},{"cell_type":"code","execution_count":null,"id":"piano-special","metadata":{"id":"piano-special"},"outputs":[],"source":["stacked_barplot(\"____________________\")  # Complete the code to plot stacked_barplot for profile_completed and status"]},{"cell_type":"markdown","id":"innocent-editor","metadata":{"id":"innocent-editor"},"source":["**After a lead shares their information by creating a profile, there may be interactions between the lead and the company to proceed with the process of enrollment. Let's see how the last activity impacts lead conversion status**"]},{"cell_type":"code","execution_count":null,"id":"suspended-memphis","metadata":{"id":"suspended-memphis"},"outputs":[],"source":["stacked_barplot(\"__________________\") # Complete the code to plot stacked_barplot for last_activity and status"]},{"cell_type":"markdown","id":"accompanied-authorization","metadata":{"id":"accompanied-authorization"},"source":["**Let's see how advertisement and referrals impact the lead status**"]},{"cell_type":"code","execution_count":null,"id":"regulated-start","metadata":{"id":"regulated-start"},"outputs":[],"source":["stacked_barplot(\"_____________________\") # Complete the code to plot stacked_barplot for print_media_type1 and status"]},{"cell_type":"code","execution_count":null,"id":"grateful-turkey","metadata":{"id":"grateful-turkey"},"outputs":[],"source":["stacked_barplot(\"___________________\") # Complete the code to plot stacked_barplot for print_media_type2 and status"]},{"cell_type":"code","execution_count":null,"id":"sorted-colony","metadata":{"id":"sorted-colony"},"outputs":[],"source":["stacked_barplot(\"___________________\") # Complete the code to plot stacked_barplot for digital_media and status"]},{"cell_type":"code","execution_count":null,"id":"nonprofit-benefit","metadata":{"id":"nonprofit-benefit"},"outputs":[],"source":["stacked_barplot(\"___________________\") # Complete the code to plot stacked_barplot for educational_channels and status"]},{"cell_type":"code","execution_count":null,"id":"historical-selling","metadata":{"id":"historical-selling"},"outputs":[],"source":["stacked_barplot(\"_________________\") # Complete the code to plot stacked_barplot for referral and status"]},{"cell_type":"markdown","id":"powerful-couple","metadata":{"id":"powerful-couple"},"source":["### Outlier Check\n","\n","- Let's check for outliers in the data."]},{"cell_type":"code","execution_count":null,"id":"imported-uganda","metadata":{"id":"imported-uganda"},"outputs":[],"source":["# outlier detection using boxplot\n","numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n","# dropping release_year as it is a temporal variable\n","numeric_columns.remove(\"status\")\n","\n","plt.figure(figsize=(15, 12))\n","\n","for i, variable in enumerate(numeric_columns):\n","    plt.subplot(4, 4, i + 1)\n","    plt.boxplot(data[variable], whis=1.5)\n","    plt.tight_layout()\n","    plt.title(variable)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"pleased-chicken","metadata":{"id":"pleased-chicken"},"source":["### Data Preparation for modeling\n","\n","- We want to predict which lead is more likely to be converted.\n","- Before we proceed to build a model, we'll have to encode categorical features.\n","- We'll split the data into train and test to be able to evaluate the model that we build on the train data."]},{"cell_type":"code","execution_count":null,"id":"durable-allergy","metadata":{"id":"durable-allergy"},"outputs":[],"source":["X = data.drop([\"status\"], axis=1)\n","Y = '________' # Complete the code to define the dependent (target) variable\n","\n","X = pd.\"_________\"(X, drop_first=True) # Complete the code to get dummies for X\n","\n","# Splitting the data in 70:30 ratio for train to test data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, Y, test_size=0.30, random_state=1\n",")"]},{"cell_type":"code","execution_count":null,"id":"ecological-principal","metadata":{"id":"ecological-principal"},"outputs":[],"source":["print(\"Shape of Training set : \", X_train.shape)\n","print(\"Shape of test set : \", X_test.shape)\n","print(\"Percentage of classes in training set:\")\n","print(y_train.value_counts(normalize=True))\n","print(\"Percentage of classes in test set:\")\n","print(y_test.value_counts(normalize=True))"]},{"cell_type":"markdown","id":"cultural-engagement","metadata":{"id":"cultural-engagement"},"source":["### Model evaluation criterion\n","\n","### Model can make wrong predictions as:\n","\n","1. Predicting a lead will not be converted to a paid customer in reality, the lead would have converted to a paid customer.\n","2. Predicting a lead will be converted to a paid customer in reality, the lead would not have converted to a paid customer.\n","\n","### Which case is more important?\n","\n","* If we predict that a lead will not get converted and the lead would have converted then the company will lose a potential customer.\n","\n","* If we predict that a lead will get converted and the lead doesn't get converted the company might lose resources by nurturing false-positive cases.\n","\n","Losing a potential customer is a greater loss.\n","\n","### How to reduce the losses?\n","\n","* Company would want `Recall` to be maximized, greater the Recall score higher are the chances of minimizing False Negatives."]},{"cell_type":"markdown","id":"ruled-appointment","metadata":{"id":"ruled-appointment"},"source":["#### First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n","* The model_performance_classification_statsmodels function will be used to check the model performance of models.\n","* The confusion_matrix_statsmodels function will be used to plot the confusion matrix."]},{"cell_type":"code","execution_count":null,"id":"prime-front","metadata":{"id":"prime-front"},"outputs":[],"source":["# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n","def model_performance_classification_statsmodels(\n","    model, predictors, target, threshold=0.5\n","):\n","    \"\"\"\n","    Function to compute different metrics to check classification model performance\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    threshold: threshold for classifying the observation as class 1\n","    \"\"\"\n","\n","    # checking which probabilities are greater than threshold\n","    pred_temp = model.predict(predictors) > threshold\n","    # rounding off the above values to get classes\n","    pred = np.round(pred_temp)\n","\n","    acc = accuracy_score(target, pred)  # to compute Accuracy\n","    recall = recall_score(target, pred)  # to compute Recall\n","    precision = precision_score(target, pred)  # to compute Precision\n","    f1 = f1_score(target, pred)  # to compute F1-score\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"code","execution_count":null,"id":"amateur-scotland","metadata":{"id":"amateur-scotland"},"outputs":[],"source":["# defining a function to plot the confusion_matrix of a classification model\n","\n","\n","def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):\n","    \"\"\"\n","    To plot the confusion_matrix with percentages\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    threshold: threshold for classifying the observation as class 1\n","    \"\"\"\n","    y_pred = model.predict(predictors) > threshold\n","    cm = confusion_matrix(target, y_pred)\n","    labels = np.asarray(\n","        [\n","            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n","            for item in cm.flatten()\n","        ]\n","    ).reshape(2, 2)\n","\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=labels, fmt=\"\")\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")"]},{"cell_type":"markdown","id":"opposed-glance","metadata":{"id":"opposed-glance"},"source":["### Logistic Regression (with statsmodels library)"]},{"cell_type":"code","execution_count":null,"id":"nominated-tumor","metadata":{"id":"nominated-tumor"},"outputs":[],"source":["X = data.drop([\"status\"], axis=1)\n","Y = data[\"status\"]\n","\n","# adding constant\n","X = sm.'_______' ## Complete the code to add constant to X\n","\n","X = pd.'_______' ## Complete the code to create dummies for X\n","\n","# Splitting data in train and test sets\n","X_train, X_test, y_train, y_test = train_test_split('_______') ## Complete the code to split the data into train test in the ratio 70:30 with random_state = 1"]},{"cell_type":"code","execution_count":null,"id":"generous-investing","metadata":{"id":"generous-investing"},"outputs":[],"source":["# fitting logistic regression model\n","logit = sm.Logit(y_train, X_train.astype(float))\n","lg = logit.fit(disp=False)\n","\n","print(lg.\"_________\") # Complete the code to get model summary"]},{"cell_type":"code","execution_count":null,"id":"raising-ultimate","metadata":{"id":"raising-ultimate"},"outputs":[],"source":["print(\"Training performance:\")\n","model_performance_classification_statsmodels(lg, X_train, y_train)"]},{"cell_type":"markdown","id":"humanitarian-macedonia","metadata":{"id":"humanitarian-macedonia"},"source":["### Multicollinearity"]},{"cell_type":"code","execution_count":null,"id":"greenhouse-tattoo","metadata":{"id":"greenhouse-tattoo"},"outputs":[],"source":["# we will define a function to check VIF\n","def checking_vif(predictors):\n","    vif = pd.DataFrame()\n","    vif[\"feature\"] = predictors.columns\n","\n","    # calculating VIF for each feature\n","    vif[\"VIF\"] = [\n","        variance_inflation_factor(predictors.values, i)\n","        for i in range(len(predictors.columns))\n","    ]\n","    return vif"]},{"cell_type":"code","execution_count":null,"id":"restricted-negative","metadata":{"id":"restricted-negative"},"outputs":[],"source":["checking_vif(X_train)"]},{"cell_type":"markdown","id":"expensive-folks","metadata":{"id":"expensive-folks"},"source":["### Dropping high p-value variables\n","\n","- We will drop the predictor variables having a p-value greater than 0.05 as they do not significantly impact the target variable.\n","- But sometimes p-values change after dropping a variable. So, we'll not drop all variables at once.\n","- Instead, we will do the following:\n","    - Build a model, check the p-values of the variables, and drop the column with the highest p-value.\n","    - Create a new model without the dropped feature, check the p-values of the variables, and drop the column with the highest p-value.\n","    - Repeat the above two steps till there are no columns with p-value > 0.05.\n","\n","The above process can also be done manually by picking one variable at a time that has a high p-value, dropping it, and building a model again. But that might be a little tedious and using a loop will be more efficient."]},{"cell_type":"code","execution_count":null,"id":"transsexual-presence","metadata":{"id":"transsexual-presence"},"outputs":[],"source":["# initial list of columns\n","cols = X_train.columns.tolist()\n","\n","# setting an initial max p-value\n","max_p_value = 1\n","\n","while len(cols) > 0:\n","    # defining the train set\n","    x_train_aux = X_train[cols]\n","\n","    # fitting the model\n","    model = sm.Logit(y_train, x_train_aux).fit(disp=False)\n","\n","    # getting the p-values and the maximum p-value\n","    p_values = model.pvalues\n","    max_p_value = max(p_values)\n","\n","    # name of the variable with maximum p-value\n","    feature_with_p_max = p_values.idxmax()\n","\n","    if max_p_value > 0.05:\n","        cols.remove(feature_with_p_max)\n","    else:\n","        break\n","\n","selected_features = cols\n","print(selected_features)"]},{"cell_type":"code","execution_count":null,"id":"vertical-diploma","metadata":{"id":"vertical-diploma"},"outputs":[],"source":["X_train1 = X_train[selected_features]\n","X_test1 = X_test[selected_features]"]},{"cell_type":"code","execution_count":null,"id":"composed-athens","metadata":{"id":"composed-athens"},"outputs":[],"source":["logit1 = sm.'_______' ## Complete the code to train logistic regression on X_train1 and y_train\n","lg1 = logit1.'_______' ## Complete the code to fit logistic regression\n","print(lg1.'_______') ## Complete the code to print summary of the model"]},{"cell_type":"code","execution_count":null,"id":"floral-daily","metadata":{"id":"floral-daily"},"outputs":[],"source":["print(\"Training performance:\")\n","model_performance_classification_statsmodels(lg1,'_______') ## Complete the code to check performance on X_train1 and y_train"]},{"cell_type":"markdown","id":"external-magnitude","metadata":{"id":"external-magnitude"},"source":["###  Converting coefficients to odds\n","* The coefficients of the logistic regression model are in terms of log(odd), to find the odds we have to take the exponential of the coefficients.\n","* Therefore, **odds =  exp(b)**\n","* The percentage change in odds is given as **odds = (exp(b) - 1) * 100**"]},{"cell_type":"code","execution_count":null,"id":"armed-omega","metadata":{"id":"armed-omega"},"outputs":[],"source":["# converting coefficients to odds\n","odds = np.exp(lg1.params)\n","\n","# finding the percentage change\n","perc_change_odds = (np.exp(lg1.params) - 1) * 100\n","\n","# removing limit from number of columns to display\n","pd.set_option(\"display.max_columns\", None)\n","\n","# adding the odds to a dataframe\n","pd.DataFrame({\"Odds\": odds, \"Change_odd%\": perc_change_odds}, index=X_train1.columns).T"]},{"cell_type":"markdown","id":"waiting-deputy","metadata":{"id":"waiting-deputy"},"source":["#### Checking model performance on the training set"]},{"cell_type":"code","execution_count":null,"id":"auburn-lambda","metadata":{"id":"auburn-lambda"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels(lg1, X_train1, y_train)"]},{"cell_type":"code","execution_count":null,"id":"stable-fabric","metadata":{"id":"stable-fabric"},"outputs":[],"source":["print(\"Training performance:\")\n","log_reg_model_train_perf = model_performance_classification_statsmodels(lg1, '_______') ## Complete the code to check performance on X_train1 and y_train\n","log_reg_model_train_perf"]},{"cell_type":"markdown","id":"polished-stewart","metadata":{"id":"polished-stewart"},"source":["#### ROC-AUC\n","* ROC-AUC on training set"]},{"cell_type":"code","execution_count":null,"id":"quarterly-arnold","metadata":{"id":"quarterly-arnold"},"outputs":[],"source":["logit_roc_auc_train = roc_auc_score(y_train, lg1.predict(X_train1))\n","fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))\n","plt.figure(figsize=(7, 5))\n","plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n","plt.plot([0, 1], [0, 1], \"r--\")\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.01])\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"Receiver operating characteristic\")\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"markdown","id":"monthly-segment","metadata":{"id":"monthly-segment"},"source":["### Model Performance Improvement"]},{"cell_type":"markdown","id":"rapid-conflict","metadata":{"id":"rapid-conflict"},"source":["* Let's see if the recall score can be improved further, by changing the model threshold using AUC-ROC Curve."]},{"cell_type":"markdown","id":"turkish-valuation","metadata":{"id":"turkish-valuation"},"source":["### Optimal threshold using AUC-ROC curve"]},{"cell_type":"code","execution_count":null,"id":"demonstrated-raise","metadata":{"id":"demonstrated-raise"},"outputs":[],"source":["# Optimal threshold as per AUC-ROC curve\n","# The optimal cut off would be where tpr is high and fpr is low\n","fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))\n","\n","optimal_idx = np.argmax(tpr - fpr)\n","optimal_threshold_auc_roc = thresholds[optimal_idx]\n","print(optimal_threshold_auc_roc)"]},{"cell_type":"code","execution_count":null,"id":"saved-plastic","metadata":{"id":"saved-plastic"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels(\n","    lg1, '______________',\n",") ## Complete the code to create the confusion matrix for X_train1 and y_train with optimal_threshold_auc_roc as threshold"]},{"cell_type":"code","execution_count":null,"id":"geological-strengthening","metadata":{"id":"geological-strengthening"},"outputs":[],"source":["# checking model performance for this model\n","log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_statsmodels(\n","    lg1, X_train1, y_train, threshold=optimal_threshold_auc_roc\n",")\n","print(\"Training performance:\")\n","log_reg_model_train_perf_threshold_auc_roc"]},{"cell_type":"markdown","id":"heated-intersection","metadata":{"id":"heated-intersection"},"source":["#### Let's use Precision-Recall curve and see if we can find a better threshold"]},{"cell_type":"code","execution_count":null,"id":"acute-newark","metadata":{"id":"acute-newark"},"outputs":[],"source":["y_scores = lg1.predict(X_train1)\n","prec, rec, tre = precision_recall_curve(y_train, y_scores,)\n","\n","\n","def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n","    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n","    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n","    plt.xlabel(\"Threshold\")\n","    plt.legend(loc=\"upper left\")\n","    plt.ylim([0, 1])\n","\n","\n","plt.figure(figsize=(10, 7))\n","plot_prec_recall_vs_tresh(prec, rec, tre)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"brown-advertiser","metadata":{"id":"brown-advertiser"},"outputs":[],"source":["# setting the threshold\n","optimal_threshold_curve = 0.38"]},{"cell_type":"markdown","id":"environmental-surface","metadata":{"id":"environmental-surface"},"source":["#### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"mental-albany","metadata":{"id":"mental-albany"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels(\n","    lg1, '______________',\n",") ## Complete the code to create the confusion matrix for X_train1 and y_train with optimal_threshold_curve as threshold"]},{"cell_type":"code","execution_count":null,"id":"fifteen-bundle","metadata":{"id":"fifteen-bundle"},"outputs":[],"source":["log_reg_model_train_perf_threshold_curve = model_performance_classification_statsmodels(\n","    lg1, X_train1, y_train, threshold=optimal_threshold_curve\n",")\n","print(\"Training performance:\")\n","log_reg_model_train_perf_threshold_curve"]},{"cell_type":"markdown","id":"organic-pocket","metadata":{"id":"organic-pocket"},"source":["### Let's check the performance on the test set"]},{"cell_type":"markdown","id":"first-governor","metadata":{"id":"first-governor"},"source":["**Using model with default threshold**"]},{"cell_type":"code","execution_count":null,"id":"visible-voluntary","metadata":{"id":"visible-voluntary"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels('_______') ## Complete the code to create confusion matrix for X_test1 and y_test"]},{"cell_type":"code","execution_count":null,"id":"biological-single","metadata":{"id":"biological-single"},"outputs":[],"source":["log_reg_model_test_perf = model_performance_classification_statsmodels('_______') ## Complete the code to check performance on X_test1 and y_test\n","\n","print(\"Test performance:\")\n","log_reg_model_test_perf"]},{"cell_type":"markdown","id":"fitted-marsh","metadata":{"id":"fitted-marsh"},"source":["* ROC curve on test set"]},{"cell_type":"code","execution_count":null,"id":"included-package","metadata":{"id":"included-package"},"outputs":[],"source":["logit_roc_auc_train = roc_auc_score(y_test, lg1.predict(X_test1))\n","fpr, tpr, thresholds = roc_curve(y_test, lg1.predict(X_test1))\n","plt.figure(figsize=(7, 5))\n","plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n","plt.plot([0, 1], [0, 1], \"r--\")\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.01])\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"Receiver operating characteristic\")\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"markdown","id":"computational-concord","metadata":{"id":"computational-concord"},"source":["**Using model with threshold=0.26**"]},{"cell_type":"code","execution_count":null,"id":"stone-wireless","metadata":{"id":"stone-wireless"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels(lg1, '_______') ## Complete the code to create confusion matrix for X_test1 and y_test using optimal_threshold_auc_roc as threshold"]},{"cell_type":"code","execution_count":null,"id":"southern-inquiry","metadata":{"id":"southern-inquiry"},"outputs":[],"source":["# checking model performance for this model\n","log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_statsmodels(\n","    lg1, X_test1, y_test, threshold=optimal_threshold_auc_roc\n",")\n","print(\"Test performance:\")\n","log_reg_model_test_perf_threshold_auc_roc"]},{"cell_type":"markdown","id":"graphic-penetration","metadata":{"id":"graphic-penetration"},"source":["**Using model with threshold = 0.38**"]},{"cell_type":"code","execution_count":null,"id":"needed-trial","metadata":{"id":"needed-trial"},"outputs":[],"source":["# creating confusion matrix\n","confusion_matrix_statsmodels(lg1, '_______') ## Complete the code to create confusion matrix for X_test1 and y_test using optimal_threshold_curve as threshold"]},{"cell_type":"code","execution_count":null,"id":"major-sucking","metadata":{"id":"major-sucking"},"outputs":[],"source":["log_reg_model_test_perf_threshold_curve = model_performance_classification_statsmodels(\n","    lg1, X_test1, y_test, threshold=optimal_threshold_curve\n",")\n","print(\"Test performance:\")\n","log_reg_model_test_perf_threshold_curve"]},{"cell_type":"markdown","id":"polish-moldova","metadata":{"id":"polish-moldova"},"source":["### Model performance summary"]},{"cell_type":"code","execution_count":null,"id":"domestic-election","metadata":{"id":"domestic-election"},"outputs":[],"source":["# training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [\n","        log_reg_model_train_perf.T,\n","        log_reg_model_train_perf_threshold_auc_roc.T,\n","        log_reg_model_train_perf_threshold_curve.T,\n","    ],\n","    axis=1,\n",")\n","models_train_comp_df.columns = [\n","    \"Logistic Regression-default Threshold\",\n","    \"Logistic Regression-0.37 Threshold\",\n","    \"Logistic Regression-0.42 Threshold\",\n","]\n","\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","execution_count":null,"id":"acute-wyoming","metadata":{"id":"acute-wyoming"},"outputs":[],"source":["# test performance comparison\n","\n","'_______' ## Complete the code to compare test performance"]},{"cell_type":"markdown","id":"separated-prague","metadata":{"id":"separated-prague"},"source":["## Decision Tree"]},{"cell_type":"code","execution_count":null,"id":"eight-class","metadata":{"id":"eight-class"},"outputs":[],"source":["X = data.drop([\"status\"], axis=1)\n","Y = data[\"status\"]\n","\n","X = pd.'_______' ## Complete the code to create dummies for X\n","\n","# Splitting data in train and test sets\n","X_train, X_test, y_train, y_test = train_test_split('_______') ## Complete the code to split the data into train test in the ratio 70:30 with random_state = 1"]},{"cell_type":"markdown","id":"employed-charger","metadata":{"id":"employed-charger"},"source":["#### First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n","* The model_performance_classification_sklearn function will be used to check the model performance of models.\n","* The confusion_matrix_sklearnfunction will be used to plot the confusion matrix."]},{"cell_type":"code","execution_count":null,"id":"industrial-parent","metadata":{"id":"industrial-parent"},"outputs":[],"source":["# defining a function to compute different metrics to check performance of a classification model built using sklearn\n","def model_performance_classification_sklearn(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check classification model performance\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    acc = accuracy_score(target, pred)  # to compute Accuracy\n","    recall = recall_score(target, pred)  # to compute Recall\n","    precision = precision_score(target, pred)  # to compute Precision\n","    f1 = f1_score(target, pred)  # to compute F1-score\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"code","execution_count":null,"id":"thrown-protein","metadata":{"id":"thrown-protein"},"outputs":[],"source":["def confusion_matrix_sklearn(model, predictors, target):\n","    \"\"\"\n","    To plot the confusion_matrix with percentages\n","\n","    model: classifier\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","    y_pred = model.predict(predictors)\n","    cm = confusion_matrix(target, y_pred)\n","    labels = np.asarray(\n","        [\n","            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n","            for item in cm.flatten()\n","        ]\n","    ).reshape(2, 2)\n","\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=labels, fmt=\"\")\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")"]},{"cell_type":"markdown","id":"immune-malta","metadata":{"id":"immune-malta"},"source":["### Building Decision Tree Model"]},{"cell_type":"code","execution_count":null,"id":"recognized-nurse","metadata":{"id":"recognized-nurse"},"outputs":[],"source":["model = DecisionTreeClassifier(random_state=1)\n","model.('_______') ## Complete the code to fit decision tree on train data"]},{"cell_type":"markdown","id":"identified-upper","metadata":{"id":"identified-upper"},"source":["#### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"female-kennedy","metadata":{"id":"female-kennedy"},"outputs":[],"source":["confusion_matrix_sklearn('_______') ## Complete the code to create confusion matrix for train data"]},{"cell_type":"code","execution_count":null,"id":"falling-squad","metadata":{"id":"falling-squad","scrolled":true},"outputs":[],"source":["decision_tree_perf_train = model_performance_classification_sklearn(\n","    model, X_train, y_train\n",")\n","decision_tree_perf_train"]},{"cell_type":"markdown","id":"neither-omaha","metadata":{"id":"neither-omaha"},"source":["#### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"8e2d5935","metadata":{"id":"8e2d5935"},"outputs":[],"source":["confusion_matrix_sklearn('_______') ## Complete the code to create confusion matrix for test data"]},{"cell_type":"code","execution_count":null,"id":"applied-magazine","metadata":{"id":"applied-magazine","scrolled":true},"outputs":[],"source":["decision_tree_perf_test = model_performance_classification_sklearn('_______') ## Complete the code to check performance on test set\n","decision_tree_perf_test"]},{"cell_type":"markdown","id":"rational-details","metadata":{"id":"rational-details"},"source":["**Before pruning the tree let's check the important features.**"]},{"cell_type":"code","execution_count":null,"id":"genetic-channels","metadata":{"id":"genetic-channels"},"outputs":[],"source":["feature_names = list(X_train.columns)\n","importances = model.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.figure(figsize=(8, 8))\n","plt.title(\"Feature Importances\")\n","plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n","plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","plt.xlabel(\"Relative Importance\")\n","plt.show()"]},{"cell_type":"markdown","id":"detailed-possible","metadata":{"id":"detailed-possible"},"source":["### Pruning the tree"]},{"cell_type":"markdown","id":"ancient-composer","metadata":{"id":"ancient-composer"},"source":["**Pre-Pruning**"]},{"cell_type":"code","execution_count":null,"id":"romantic-stationery","metadata":{"id":"romantic-stationery"},"outputs":[],"source":["# Choose the type of classifier.\n","estimator = DecisionTreeClassifier(random_state=1, class_weight={0: 0.3, 1: 0.7})\n","\n","# Grid of parameters to choose from\n","parameters = {\n","    \"max_depth\": np.arange(5, 13, 2),\n","    \"max_leaf_nodes\": [10, 20, 40, 50, 75, 100],\n","    \"min_samples_split\": [2, 5, 7, 10, 20, 30],\n","}\n","\n","# Type of scoring used to compare parameter combinations\n","acc_scorer = make_scorer(recall_score)\n","\n","# Run the grid search\n","grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)\n","grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# Set the clf to the best combination of parameters\n","estimator = grid_obj.best_estimator_\n","\n","# Fit the best algorithm to the data.\n","estimator.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"endangered-image","metadata":{"id":"endangered-image"},"source":["#### Checking performance on training set"]},{"cell_type":"code","execution_count":null,"id":"skilled-poster","metadata":{"id":"skilled-poster"},"outputs":[],"source":["confusion_matrix_sklearn('_______') ## Complete the code to create confusion matrix for train data"]},{"cell_type":"code","execution_count":null,"id":"detected-folks","metadata":{"id":"detected-folks"},"outputs":[],"source":["decision_tree_tune_perf_train = model_performance_classification_sklearn('_______') ## Complete the code to check performance on train set\n","decision_tree_tune_perf_train"]},{"cell_type":"markdown","id":"concrete-season","metadata":{"id":"concrete-season"},"source":["#### Checking performance on test set"]},{"cell_type":"code","execution_count":null,"id":"banner-comparative","metadata":{"id":"banner-comparative"},"outputs":[],"source":["confusion_matrix_sklearn('_______') ## Complete the code to create confusion matrix for test data"]},{"cell_type":"code","execution_count":null,"id":"killing-magnet","metadata":{"id":"killing-magnet"},"outputs":[],"source":["decision_tree_tune_perf_test = model_performance_classification_sklearn('_______') ## Complete the code to check performance on test set\n","decision_tree_tune_perf_test"]},{"cell_type":"markdown","id":"frequent-grenada","metadata":{"id":"frequent-grenada"},"source":["### Visualizing the Decision Tree"]},{"cell_type":"code","execution_count":null,"id":"driving-state","metadata":{"id":"driving-state"},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","out = tree.plot_tree(\n","    estimator,\n","    feature_names=feature_names,\n","    filled=True,\n","    fontsize=9,\n","    node_ids=False,\n","    class_names=None,\n",")\n","# below code will add arrows to the decision tree split if they are missing\n","for o in out:\n","    arrow = o.arrow_patch\n","    if arrow is not None:\n","        arrow.set_edgecolor(\"black\")\n","        arrow.set_linewidth(1)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"narrow-johns","metadata":{"id":"narrow-johns","scrolled":true},"outputs":[],"source":["# Text report showing the rules of a decision tree -\n","print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))"]},{"cell_type":"code","execution_count":null,"id":"secure-killing","metadata":{"id":"secure-killing"},"outputs":[],"source":["# importance of features in the tree building\n","\n","importances = estimator.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.figure(figsize=(8, 8))\n","plt.title(\"Feature Importances\")\n","plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n","plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","plt.xlabel(\"Relative Importance\")\n","plt.show()"]},{"cell_type":"markdown","id":"based-recruitment","metadata":{"id":"based-recruitment"},"source":["**Cost Complexity Pruning**"]},{"cell_type":"code","execution_count":null,"id":"white-advocacy","metadata":{"id":"white-advocacy"},"outputs":[],"source":["clf = DecisionTreeClassifier(random_state=1)\n","path = clf.cost_complexity_pruning_path(X_train, y_train)\n","ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities"]},{"cell_type":"code","execution_count":null,"id":"associate-audit","metadata":{"id":"associate-audit"},"outputs":[],"source":["pd.DataFrame(path)"]},{"cell_type":"code","execution_count":null,"id":"numeric-internet","metadata":{"id":"numeric-internet"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 5))\n","ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n","ax.set_xlabel(\"effective alpha\")\n","ax.set_ylabel(\"total impurity of leaves\")\n","ax.set_title(\"Total Impurity vs effective alpha for training set\")\n","plt.show()"]},{"cell_type":"markdown","id":"reasonable-coral","metadata":{"id":"reasonable-coral"},"source":["Next, we train a decision tree using effective alphas. The last value\n","in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n","leaving the tree, ``clfs[-1]``, with one node."]},{"cell_type":"code","execution_count":null,"id":"stone-causing","metadata":{"id":"stone-causing"},"outputs":[],"source":["clfs = []\n","for ccp_alpha in ccp_alphas:\n","    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n","    clf.'_______' ## Complete the code to fit decision tree on training data\n","    clfs.append(clf)\n","print(\n","    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n","        clfs[-1].tree_.node_count, ccp_alphas[-1]\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"id":"statistical-palace","metadata":{"id":"statistical-palace"},"outputs":[],"source":["clfs = clfs[:-1]\n","ccp_alphas = ccp_alphas[:-1]\n","\n","node_counts = [clf.tree_.node_count for clf in clfs]\n","depth = [clf.tree_.max_depth for clf in clfs]\n","fig, ax = plt.subplots(2, 1, figsize=(10, 7))\n","ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n","ax[0].set_xlabel(\"alpha\")\n","ax[0].set_ylabel(\"number of nodes\")\n","ax[0].set_title(\"Number of nodes vs alpha\")\n","ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n","ax[1].set_xlabel(\"alpha\")\n","ax[1].set_ylabel(\"depth of tree\")\n","ax[1].set_title(\"Depth vs alpha\")\n","fig.tight_layout()"]},{"cell_type":"markdown","id":"wooden-subsection","metadata":{"id":"wooden-subsection"},"source":["### Recall Score vs alpha for training and testing sets"]},{"cell_type":"code","execution_count":null,"id":"abandoned-danger","metadata":{"id":"abandoned-danger"},"outputs":[],"source":["recall_train = []\n","for clf in clfs:\n","    pred_train = clf.predict(X_train)\n","    values_train = recall_score(y_train, pred_train)\n","    recall_train.append(values_train)\n","\n","recall_test = []\n","for clf in clfs:\n","    pred_test = clf.predict(X_test)\n","    values_test = recall_score(y_test, pred_test)\n","    recall_test.append(values_test)"]},{"cell_type":"code","execution_count":null,"id":"golden-grenada","metadata":{"id":"golden-grenada"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15, 5))\n","ax.set_xlabel(\"alpha\")\n","ax.set_ylabel(\"F1 Score\")\n","ax.set_title(\"F1 Score vs alpha for training and testing sets\")\n","ax.plot(ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n","ax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"extreme-inspiration","metadata":{"id":"extreme-inspiration"},"outputs":[],"source":["index_best_model = np.argmax(recall_test)\n","best_model = clfs[index_best_model]\n","print(best_model)"]},{"cell_type":"markdown","id":"bearing-illinois","metadata":{"id":"bearing-illinois"},"source":["#### Checking performance on training set"]},{"cell_type":"code","execution_count":null,"id":"closing-natural","metadata":{"id":"closing-natural"},"outputs":[],"source":["confusion_matrix_sklearn(best_model, X_train, y_train)"]},{"cell_type":"code","execution_count":null,"id":"hired-beauty","metadata":{"id":"hired-beauty"},"outputs":[],"source":["decision_tree_post_perf_train = model_performance_classification_sklearn(\n","    best_model, X_train, y_train\n",")\n","decision_tree_post_perf_train"]},{"cell_type":"markdown","id":"smoking-invitation","metadata":{"id":"smoking-invitation"},"source":["#### Checking performance on test set"]},{"cell_type":"code","execution_count":null,"id":"blocked-bracket","metadata":{"id":"blocked-bracket"},"outputs":[],"source":["'_______' ## Complete the code to create confusion matrix for test data on best model\n"]},{"cell_type":"code","execution_count":null,"id":"absent-transsexual","metadata":{"id":"absent-transsexual"},"outputs":[],"source":["decision_tree_post_test = '_______' ## Complete the code to check performance of test set on best model\n","decision_tree_post_test"]},{"cell_type":"code","execution_count":null,"id":"classified-banana","metadata":{"id":"classified-banana"},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","\n","out = tree.plot_tree(\n","    best_model,\n","    feature_names=feature_names,\n","    filled=True,\n","    fontsize=9,\n","    node_ids=False,\n","    class_names=None,\n",")\n","for o in out:\n","    arrow = o.arrow_patch\n","    if arrow is not None:\n","        arrow.set_edgecolor(\"black\")\n","        arrow.set_linewidth(1)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"offshore-colors","metadata":{"id":"offshore-colors"},"outputs":[],"source":["# Text report showing the rules of a decision tree -\n","\n","print(tree.export_text(best_model, feature_names=feature_names, show_weights=True))"]},{"cell_type":"code","execution_count":null,"id":"crucial-disclaimer","metadata":{"id":"crucial-disclaimer"},"outputs":[],"source":["importances = best_model.feature_importances_\n","indices = np.argsort(importances)\n","\n","plt.figure(figsize=(8, 8))\n","plt.title(\"Feature Importances\")\n","plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n","plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n","plt.xlabel(\"Relative Importance\")\n","plt.show()"]},{"cell_type":"markdown","id":"specific-columbus","metadata":{"id":"specific-columbus"},"source":["### Comparing Decision Tree models"]},{"cell_type":"code","execution_count":null,"id":"superior-reality","metadata":{"id":"superior-reality"},"outputs":[],"source":["# training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [\n","        decision_tree_perf_train.T,\n","        decision_tree_tune_perf_train.T,\n","        decision_tree_post_perf_train.T,\n","    ],\n","    axis=1,\n",")\n","models_train_comp_df.columns = [\n","    \"Decision Tree sklearn\",\n","    \"Decision Tree (Pre-Pruning)\",\n","    \"Decision Tree (Post-Pruning)\",\n","]\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","execution_count":null,"id":"working-employment","metadata":{"id":"working-employment"},"outputs":[],"source":["# testing performance comparison\n","\n","'_______' ## Complete the code to compare performance of test set"]},{"cell_type":"markdown","id":"b52492fa","metadata":{"id":"b52492fa"},"source":["### Business Recommendations"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}