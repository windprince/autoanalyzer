{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrSkDfGgDu3F"
   },
   "source": [
    "# Supervised Learning - Foundations Project: ReCell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_Ho8VGKDu3S"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_GXJwuhmVz"
   },
   "source": [
    "### Business Context\n",
    "\n",
    "Buying and selling used phones and tablets used to be something that happened on a handful of online marketplace sites. But the used and refurbished device market has grown considerably over the past decade, and a new IDC (International Data Corporation) forecast predicts that the used phone market would be worth \\\\$52.7bn by 2023 with a compound annual growth rate (CAGR) of 13.6% from 2018 to 2023. This growth can be attributed to an uptick in demand for used phones and tablets that offer considerable savings compared with new models.\n",
    "\n",
    "Refurbished and used devices continue to provide cost-effective alternatives to both consumers and businesses that are looking to save money when purchasing one. There are plenty of other benefits associated with the used device market. Used and refurbished devices can be sold with warranties and can also be insured with proof of purchase. Third-party vendors/platforms, such as Verizon, Amazon, etc., provide attractive offers to customers for refurbished devices. Maximizing the longevity of devices through second-hand trade also reduces their environmental impact and helps in recycling and reducing waste. The impact of the COVID-19 outbreak may further boost this segment as consumers cut back on discretionary spending and buy phones and tablets only for immediate needs.\n",
    "\n",
    " \n",
    "### Objective\n",
    "\n",
    "The rising potential of this comparatively under-the-radar market fuels the need for an ML-based solution to develop a dynamic pricing strategy for used and refurbished devices. ReCell, a startup aiming to tap the potential in this market, has hired you as a data scientist. They want you to analyze the data provided and build a linear regression model to predict the price of a used phone/tablet and identify factors that significantly influence it.\n",
    "\n",
    " \n",
    "### Data Description\n",
    "\n",
    "The data contains the different attributes of used/refurbished phones and tablets. The data was collected in the year 2021. The detailed data dictionary is given below.\n",
    "\n",
    "\n",
    "- brand_name: Name of manufacturing brand\n",
    "- os: OS on which the device runs\n",
    "- screen_size: Size of the screen in cm\n",
    "- 4g: Whether 4G is available or not\n",
    "- 5g: Whether 5G is available or not\n",
    "- main_camera_mp: Resolution of the rear camera in megapixels\n",
    "- selfie_camera_mp: Resolution of the front camera in megapixels\n",
    "- int_memory: Amount of internal memory (ROM) in GB\n",
    "- ram: Amount of RAM in GB\n",
    "- battery: Energy capacity of the device battery in mAh\n",
    "- weight: Weight of the device in grams\n",
    "- release_year: Year when the device model was released\n",
    "- days_used: Number of days the used/refurbished device has been used\n",
    "- normalized_new_price: Normalized price of a new device of the same model in euros\n",
    "- normalized_used_price: Normalized price of the used/refurbished device in euros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xIMk1hXrabC"
   },
   "source": [
    "## **Please read the instructions carefully before starting the project.** \n",
    "\n",
    "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned. \n",
    "\n",
    "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
    "\n",
    "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
    "\n",
    "* Identify the task to be performed correctly and only then proceed to write the required code\n",
    "\n",
    "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
    "\n",
    "* Running incomplete code may throw an error\n",
    "\n",
    "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
    "\n",
    "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_-uuGqH-qTt"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeF8YaNKDPyK"
   },
   "outputs": [],
   "source": [
    "# this will help in making the Python code more structured automatically (good coding practice)\n",
    "%load_ext nb_black\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to build linear regression_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# to check model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# to build linear regression_model using statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# to compute VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxhpZv9y-qTw"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4id2YLjhmV3"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = pd.read_csv('_______') ## Complete the code to read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aodMV5D3-qTy"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-5jUOgu-qTz"
   },
   "source": [
    "The initial steps to get an overview of any dataset is to: \n",
    "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
    "- get information about the number of rows and columns in the dataset\n",
    "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
    "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlzqMR1K-qTz"
   },
   "source": [
    "### Displaying the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNvXgPEhWaLv"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQi5ygTC-qT1"
   },
   "source": [
    "### Checking the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsQmSVQDWdQB"
   },
   "outputs": [],
   "source": [
    "data.'_______' ## Complete the code to get the shape of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TcqcxbK-qT3"
   },
   "source": [
    "### Checking the data types of the columns for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sMqr8mnWf2v"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUCorhch-qT4"
   },
   "source": [
    "### Statistical summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXcbwEJcWWWI"
   },
   "outputs": [],
   "source": [
    "data.'_______' ## Complete the code to print the statistical summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNr4bWoM-qT5"
   },
   "source": [
    "### Checking for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCydbTEHWmMl"
   },
   "outputs": [],
   "source": [
    "data.'_______' ## Complete the code to check duplicate entries in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch_TjRfF-qT5"
   },
   "source": [
    "### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c10B9HMNWq9K"
   },
   "outputs": [],
   "source": [
    "data.'_______' ## Complete the code to check duplicate entries in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4HfMUSiWxBT"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data so that original data remains unchanged\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhPuzWO7hmV8"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxiNWC9JhmV8"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opT6eO8qhmV8"
   },
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (15,10))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I7psbWYhmWD"
   },
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 2, 6))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 2, 6))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n],\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woJI2SC5hmV9"
   },
   "source": [
    "**`normalized_used_price`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOz9j7M_hmV9"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(df, \"normalized_used_price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYIpGoRWhmV9"
   },
   "source": [
    "**`normalized_new_price`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_AVTS9-hmV_"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'normalized_new_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVuQ8smPhmV_"
   },
   "source": [
    "**`screen_size`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDcR4t5uhmV_"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'screen_size'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeDwMOz6hmV_"
   },
   "source": [
    "**`main_camera_mp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZHkTniOhmWA"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'main_camera_mp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZav4AxrhmWA"
   },
   "source": [
    "**`selfie_camera_mp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxX6OlLehmWA"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'selfie_camera_mp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFXxdInchmWA"
   },
   "source": [
    "**`int_memory`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhrZ64l_hmWA"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'int_memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXF46221hmWA"
   },
   "source": [
    "**`ram`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TfjASGThmWA"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'ram'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTZI2sO1hmWB"
   },
   "source": [
    "**`weight`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDxaAIDZhmWB"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'weight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXgPSxGbhmWB"
   },
   "source": [
    "**`battery`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wo--kEQhmWB"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'battery'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WPI4QMjhmWB"
   },
   "source": [
    "**`days_used`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3JU7JxehmWB"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('_______')  ## Complete the code to create histogram_boxplot for 'days_used'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utY4IudWhmWD"
   },
   "source": [
    "**`brand_name`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VvbAPpYhmWD"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(df, \"brand_name\", perc=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIqtv7SHhmWD"
   },
   "source": [
    "**`os`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG7BdJ30hmWD"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for 'os'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rGNYyuahmWD"
   },
   "source": [
    "**`4g`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtGMz_aEhmWD"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for '4g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k69m1GPhmWD"
   },
   "source": [
    "**`5g`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5s9SA49hmWE"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for '5g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTnwoEZ6hmWE"
   },
   "source": [
    "**`release_year`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgV3q1tUhmWE"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for 'release_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga_huJrnhmWE"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc8kJMd6VxHC"
   },
   "source": [
    "**Correlation Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpJxawjAhmWE"
   },
   "outputs": [],
   "source": [
    "cols_list = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# dropping release_year as it is a temporal variable\n",
    "cols_list.remove(\"release_year\")\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(\n",
    "    df[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLjArqLFhmWF"
   },
   "source": [
    "**The amount of RAM is important for the smooth functioning of a device. Let's see how the amount of RAM varies across brands.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q77kiPCxhmWF"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(data=df, x=\"brand_name\", y=\"ram\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeDTwk21hmWF"
   },
   "source": [
    "**People who travel frequently require devices with large batteries to run through the day. But large battery often increases weight, making it feel uncomfortable in the hands. Let's create a new dataframe of only those devices which offer a large battery and analyze.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cbcd1vbZhmWF"
   },
   "outputs": [],
   "source": [
    "df_large_battery = df[df.battery > 4500]\n",
    "df_large_battery.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcrj-ImghmWF"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot('______') ## Complete the code to create a boxplot for 'brand_name' and 'weight'\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9o5QoShhmWF"
   },
   "source": [
    "**People who buy phones and tablets primarily for entertainment purposes prefer a large screen as they offer a better viewing experience. Let's create a new dataframe of only those devices which are suitable for such people and analyze.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1DT9SDBhmWF"
   },
   "outputs": [],
   "source": [
    "df_large_screen = df[df.screen_size > 6 * 2.54]\n",
    "df_large_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkSkJ0GIhmWG"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for 'brand_name' in large screen dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ4IWWn7hmWG"
   },
   "source": [
    "**Everyone likes a good camera to capture their favorite moments with loved ones. Some customers specifically look for good front cameras to click cool selfies. Let's create a new dataframe of only those devices which are suitable for this customer segment and analyze.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAond0uQhmWG"
   },
   "outputs": [],
   "source": [
    "df_selfie_camera = df[df.selfie_camera_mp > 8]\n",
    "df_selfie_camera.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdP27nZBhmWG"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for 'brand_name' in high selfie camera dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmFNBDmxhmWH"
   },
   "source": [
    "**Let's do a similar analysis for rear cameras.**\n",
    "\n",
    "- Rear cameras generally have a better resolution than front cameras, so we set the threshold higher for them at 16MP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xoY4afZhmWH"
   },
   "outputs": [],
   "source": [
    "df_main_camera = df[df.main_camera_mp > 16]\n",
    "df_main_camera.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gegyv89HhmWH"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('_______') ## Complete the code to create labeled_barplot for 'brand_name' in high main camera dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDx7dJtEhmWH"
   },
   "source": [
    "**Let's see how the price of used devices varies across the years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrdhOpHthmWH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.lineplot('_____') ## Complete the code to create a lineplot for release year and used price\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvLSkuZ2hmWI"
   },
   "source": [
    "**Let's check how the prices vary for used phones and tablets offering 4G and 5G networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5ni9yVPhmWI"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.boxplot(data=df, x=\"4g\", y=\"normalized_used_price\")\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.boxplot(data=df, x=\"5g\", y=\"normalized_used_price\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbDJ_Vl8hmWI"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jnj657ShmWI"
   },
   "source": [
    "### Missing Value Imputation\n",
    "\n",
    "- We will impute the missing values in the data by the column medians grouped by `release_year` and `brand_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzp0-E7rKffM"
   },
   "outputs": [],
   "source": [
    "# let's create a copy of the data\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QfWi8PYhmWI"
   },
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "df1.'_______' ## Complete the code to check missing values in all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQEug5qNhmWJ"
   },
   "outputs": [],
   "source": [
    "cols_impute = [\n",
    "    \"main_camera_mp\",\n",
    "    \"selfie_camera_mp\",\n",
    "    \"int_memory\",\n",
    "    \"ram\",\n",
    "    \"battery\",\n",
    "    \"weight\",\n",
    "]\n",
    "\n",
    "for col in cols_impute:\n",
    "    df1[col] = df1[col].fillna(\n",
    "        value=df1.groupby(['_____'])[col].transform(\"median\")\n",
    "    )   ## Complete the code to impute missing values in cols_impute with median by grouping the data on release year and brand name \n",
    "\n",
    "# checking for missing values\n",
    "df1.'_______' ## Complete the code to check missing values after imputing the above columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF_SHRjPhmWJ"
   },
   "source": [
    "- We will impute the remaining missing values in the data by the column medians grouped by `brand_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzHBI4C6hmWJ"
   },
   "outputs": [],
   "source": [
    "cols_impute = [\n",
    "    \"main_camera_mp\",\n",
    "    \"selfie_camera_mp\",\n",
    "    \"battery\",\n",
    "    \"weight\",\n",
    "]\n",
    "\n",
    "for col in cols_impute:\n",
    "    df1[col] = df1[col].fillna(\n",
    "        value=df1.groupby(['_____'])[col].transform(\"median\")\n",
    "    ) ## Complete the code to impute the missing values in cols_impute with median by grouping the data on brand name\n",
    "\n",
    "# checking for missing values\n",
    "df1.'_______' ## Complete the code to check missing values after imputing the above columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6JWbAyQhmWJ"
   },
   "source": [
    "- We will fill the remaining missing values in the `main_camera_mp` column by the column median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQJgQmzohmWJ"
   },
   "outputs": [],
   "source": [
    "df1[\"main_camera_mp\"] = df1[\"main_camera_mp\"].fillna(df1[\"main_camera_mp\"].'_______') ## Complete the code to impute the data with median\n",
    "\n",
    "# checking for missing values\n",
    "df1.'_______' ## Complete the code to check missing values after imputing the above columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPB6dxxmWq1s"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYOa-FxlhmWG"
   },
   "source": [
    "\n",
    "\n",
    "- Let's create a new column `years_since_release` from the `release_year` column.\n",
    "- We will consider the year of data collection, 2021, as the baseline.\n",
    "- We will drop the `release_year` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihTY3XRkhmWG"
   },
   "outputs": [],
   "source": [
    "df1[\"years_since_release\"] = 2021 - df1[\"release_year\"]\n",
    "df1.drop(\"release_year\", axis=1, inplace=True)\n",
    "df1[\"years_since_release\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jUrmT40Wx7M"
   },
   "source": [
    "### Outlier Check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLoHqqf1hmWJ"
   },
   "source": [
    "\n",
    "- Let's check for outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RSTq3PWhmWJ"
   },
   "outputs": [],
   "source": [
    "# outlier detection using boxplot\n",
    "num_cols = df1.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, variable in enumerate(num_cols):\n",
    "    plt.subplot(4, 3, i + 1)\n",
    "    sns.boxplot(data=df1, x=variable)\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QChrTYFaKvTk"
   },
   "source": [
    "### Data Preparation for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwu8iCRQhmWK"
   },
   "source": [
    "- We want to predict the normalized price of used devices\n",
    "- Before we proceed to build a model, we'll have to encode categorical features\n",
    "- We'll split the data into train and test to be able to evaluate the model that we build on the train data\n",
    "- We will build a Linear Regression model using the train data and then check it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "372WHRhJGpj4"
   },
   "outputs": [],
   "source": [
    "## Complete the code to define the dependent and independent variables\n",
    "X = '_______'\n",
    "y = '_______'\n",
    "\n",
    "print(X.head())\n",
    "print()\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py6xQDcfLQLk"
   },
   "outputs": [],
   "source": [
    "# let's add the intercept to data\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWw6sT0yhmWK"
   },
   "outputs": [],
   "source": [
    "# creating dummy variables\n",
    "X = pd.'_______'(\n",
    "    X,\n",
    "    columns=X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n",
    "    drop_first=True,\n",
    ")  ## Complete the code to create dummies for independent features\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBXfTtAPGoaF"
   },
   "outputs": [],
   "source": [
    "# splitting the data in 70:30 ratio for train to test data\n",
    "\n",
    "x_train, x_test, y_train, y_test = '_______' ## Complete the code to split the data into train and test in specified ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUGp866qhmWK"
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows in train data =\", x_train.shape[0])\n",
    "print(\"Number of rows in test data =\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JsqBJEohmWK"
   },
   "source": [
    "## Model Building - Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uylJTNMqH8b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "olsmodel1 = sm.'_______' ## Complete the code to fit OLS model\n",
    "print(olsmodel1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dgIbf7ALlYm"
   },
   "source": [
    "### Model Performance Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MyYq4vaKffS"
   },
   "source": [
    "**Let's check the performance of the model using different metrics.**\n",
    "\n",
    "* We will be using metric functions defined in sklearn for RMSE, MAE, and $R^2$.\n",
    "* We will define a function to calculate MAPE and adjusted $R^2$.    \n",
    "* We will create a function which will print out all the above metrics in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deaapIIvlJIL"
   },
   "outputs": [],
   "source": [
    "# function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# function to compute MAPE\n",
    "def mape_score(targets, predictions):\n",
    "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # to compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
    "    mape = mape_score(target, pred)  # to compute MAPE\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVBBaPtf-BZd"
   },
   "outputs": [],
   "source": [
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmodel1_train_perf = model_performance_regression(olsmodel1, x_train, y_train)\n",
    "olsmodel1_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv51vNBhlJIL"
   },
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmodel1_test_perf = model_performance_regression('_______') ## Complete the code to check the performance on test data\n",
    "olsmodel1_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9GxSQf-qH8e"
   },
   "source": [
    "## Checking Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wr6XkwoqH8f"
   },
   "source": [
    "We will be checking the following Linear Regression assumptions:\n",
    "\n",
    "1. **No Multicollinearity**\n",
    "\n",
    "2. **Linearity of variables**\n",
    "\n",
    "3. **Independence of error terms**\n",
    "\n",
    "4. **Normality of error terms**\n",
    "\n",
    "5. **No Heteroscedasticity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB4GanGWSOBT"
   },
   "source": [
    "### TEST FOR MULTICOLLINEARITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mROHQdzZqH8f"
   },
   "source": [
    "\n",
    "- We will test for multicollinearity using VIF.\n",
    "\n",
    "- **General Rule of thumb**:\n",
    "    - If VIF is 1 then there is no correlation between the $k$th predictor and the remaining predictor variables.\n",
    "    - If VIF exceeds 5 or is close to exceeding 5, we say there is moderate multicollinearity.\n",
    "    - If VIF is 10 or exceeding 10, it shows signs of high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm6Y_e-mTvDl"
   },
   "source": [
    "Let's define a function to check VIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV-69rVWqH8g"
   },
   "outputs": [],
   "source": [
    "#Let's define a function to check VIF.\n",
    "\n",
    "def checking_vif(predictors):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = predictors.columns\n",
    "\n",
    "    # calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(predictors.values, i)\n",
    "        for i in range(len(predictors.columns))\n",
    "    ]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPdvX4xZhmWL"
   },
   "outputs": [],
   "source": [
    "checking_vif('_______')  ## Complete the code to check VIF on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulUc-ceISeyu"
   },
   "source": [
    "### Removing Multicollinearity (if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSiS4SRaqH8q"
   },
   "source": [
    "\n",
    "To remove multicollinearity\n",
    "\n",
    "1. Drop every column one by one that has a VIF score greater than 5.\n",
    "2. Look at the adjusted R-squared and RMSE of all these models.\n",
    "3. Drop the variable that makes the least change in adjusted R-squared.\n",
    "4. Check the VIF scores again.\n",
    "5. Continue till you get all VIF scores under 5.\n",
    "\n",
    "Let's define a function that will help us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93U9udWjKffT"
   },
   "outputs": [],
   "source": [
    "def treating_multicollinearity(predictors, target, high_vif_columns):\n",
    "    \"\"\"\n",
    "    Checking the effect of dropping the columns showing high multicollinearity\n",
    "    on model performance (adj. R-squared and RMSE)\n",
    "\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    high_vif_columns: columns having high VIF\n",
    "    \"\"\"\n",
    "    # empty lists to store adj. R-squared and RMSE values\n",
    "    adj_r2 = []\n",
    "    rmse = []\n",
    "\n",
    "    # build ols models by dropping one of the high VIF columns at a time\n",
    "    # store the adjusted R-squared and RMSE in the lists defined previously\n",
    "    for cols in high_vif_columns:\n",
    "        # defining the new train set\n",
    "        train = predictors.loc[:, ~predictors.columns.str.startswith(cols)]\n",
    "\n",
    "        # create the model\n",
    "        olsmodel = sm.OLS(target, train).fit()\n",
    "\n",
    "        # adding adj. R-squared and RMSE to the lists\n",
    "        adj_r2.append(olsmodel.rsquared_adj)\n",
    "        rmse.append(np.sqrt(olsmodel.mse_resid))\n",
    "\n",
    "    # creating a dataframe for the results\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            \"col\": high_vif_columns,\n",
    "            \"Adj. R-squared after_dropping col\": adj_r2,\n",
    "            \"RMSE after dropping col\": rmse,\n",
    "        }\n",
    "    ).sort_values(by=\"Adj. R-squared after_dropping col\", ascending=False)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r08PiCLiKffT"
   },
   "outputs": [],
   "source": [
    "col_list = ['____'] ## Complete the code to specify the columns with high VIF\n",
    "\n",
    "res = treating_multicollinearity('_____', y_train, col_list) ## Complete the code to check the effect on model performance after dropping specified columns from train data\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDJlsR2dKffU"
   },
   "outputs": [],
   "source": [
    "col_to_drop = '_____' ## Complete the code to specify the column to drop\n",
    "x_train2 = '_____'.loc[:, ~'_____'.columns.str.startswith(col_to_drop)] ## Complete the code to specify the train data from which to drop the column specified\n",
    "x_test2 = '_____'.loc[:, ~'_____'.columns.str.startswith(col_to_drop)] ## Complete the code to specify the test data from which to drop the column specified\n",
    "\n",
    "# Check VIF now\n",
    "vif = checking_vif(x_train2)\n",
    "print(\"VIF after dropping \", col_to_drop)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2hb9VujhmWL"
   },
   "source": [
    "### Dropping high p-value variables (if needed)\n",
    "\n",
    "- We will drop the predictor variables having a p-value greater than 0.05 as they do not significantly impact the target variable.\n",
    "- But sometimes p-values change after dropping a variable. So, we'll not drop all variables at once.\n",
    "- Instead, we will do the following:\n",
    "    - Build a model, check the p-values of the variables, and drop the column with the highest p-value.\n",
    "    - Create a new model without the dropped feature, check the p-values of the variables, and drop the column with the highest p-value.\n",
    "    - Repeat the above two steps till there are no columns with p-value > 0.05.\n",
    "\n",
    "The above process can also be done manually by picking one variable at a time that has a high p-value, dropping it, and building a model again. But that might be a little tedious and using a loop will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTFISTQlSy6K"
   },
   "outputs": [],
   "source": [
    "# initial list of columns\n",
    "predictors = '_____'.copy()  ## Complete the code to check for p-values on the right dataset\n",
    "cols = predictors.columns.tolist()\n",
    "\n",
    "# setting an initial max p-value\n",
    "max_p_value = 1\n",
    "\n",
    "while len(cols) > 0:\n",
    "    # defining the train set\n",
    "    x_train_aux = predictors[cols]\n",
    "\n",
    "    # fitting the model\n",
    "    model = sm.OLS(y_train, x_train_aux).fit()\n",
    "\n",
    "    # getting the p-values and the maximum p-value\n",
    "    p_values = model.pvalues\n",
    "    max_p_value = max(p_values)\n",
    "\n",
    "    # name of the variable with maximum p-value\n",
    "    feature_with_p_max = p_values.idxmax()\n",
    "\n",
    "    if max_p_value > 0.05:\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features = cols\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CfofLchmWL"
   },
   "outputs": [],
   "source": [
    "x_train3 = '____'[selected_features]  ## Complete the code to specify the train data from which to select the specified columns\n",
    "x_test3 = '____'[selected_features]  ## Complete the code to specify the test data from which to select the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaaUG3jDhmWM"
   },
   "outputs": [],
   "source": [
    "olsmodel2 = sm.'_______' ## Complete the code fit OLS() on updated dataset (no multicollinearity and no insignificant predictors)\n",
    "print(olsmodel2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgmwbtRXhmWM"
   },
   "outputs": [],
   "source": [
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmodel2_train_perf = model_performance_regression('_______') ## Complete the code to check performance on train data\n",
    "olsmodel2_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xMPZ9pRhmWM"
   },
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmodel2_test_perf = model_performance_regression('_______') ## Complete the code to check performance on test data\n",
    "olsmodel2_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctk29PsRqH90"
   },
   "source": [
    "**Now we'll check the rest of the assumptions on *olsmod2*.**\n",
    "\n",
    "2. **Linearity of variables**\n",
    "\n",
    "3. **Independence of error terms**\n",
    "\n",
    "4. **Normality of error terms**\n",
    "\n",
    "5. **No Heteroscedasticity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykvaeUZ-TkjD"
   },
   "source": [
    "### TEST FOR LINEARITY AND INDEPENDENCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnOY6E5uqH-C"
   },
   "source": [
    "\n",
    "- We will test for linearity and independence by making a plot of fitted values vs residuals and checking for patterns.\n",
    "- If there is no pattern, then we say the model is linear and residuals are independent.\n",
    "- Otherwise, the model is showing signs of non-linearity and residuals are not independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNkYG_qihmWM"
   },
   "outputs": [],
   "source": [
    "# let us create a dataframe with actual, fitted and residual values\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "df_pred[\"Actual Values\"] = y_train  # actual values\n",
    "df_pred[\"Fitted Values\"] = olsmodel2.fittedvalues  # predicted values\n",
    "df_pred[\"Residuals\"] = olsmodel2.resid  # residuals\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ4ivHQiqH-G"
   },
   "outputs": [],
   "source": [
    "# let's plot the fitted values vs residuals\n",
    "\n",
    "sns.residplot(\n",
    "    data=df_pred, x=\"Fitted Values\", y=\"Residuals\", color=\"purple\", lowess=True\n",
    ")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Fitted vs Residual plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOxf_qsQqH-N"
   },
   "source": [
    "### TEST FOR NORMALITY\n",
    "\n",
    "- We will test for normality by checking the distribution of residuals, by checking the Q-Q plot of residuals, and by using the Shapiro-Wilk test.\n",
    "- If the residuals follow a normal distribution, they will make a straight line plot, otherwise not.\n",
    "- If the p-value of the Shapiro-Wilk test is greater than 0.05, we can say the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUdQ6XJLqH-N"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df_pred, '_______') ## Complete the code to plot the distribution of residuals\n",
    "plt.title(\"Normality of residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj8y7-1nqH-Q"
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot('_____', dist=\"norm\", plot=pylab) ## Complete the code check Q-Q plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1LIATpBqH-U"
   },
   "outputs": [],
   "source": [
    "stats.shapiro('_______') ## Complete the code to apply the Shapiro-Wilks test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTnuLaLBXvdR"
   },
   "source": [
    "### TEST FOR HOMOSCEDASTICITY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gZ4GKY_qH-Z"
   },
   "source": [
    "\n",
    "- We will test for homoscedasticity by using the goldfeldquandt test.\n",
    "- If we get a p-value greater than 0.05, we can say that the residuals are homoscedastic. Otherwise, they are heteroscedastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naX-iXItqH-b"
   },
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "name = [\"F statistic\", \"p-value\"]\n",
    "test = sms.het_goldfeldquandt(df_pred[\"Residuals\"], '_____') ## Complete the code with the right train data to apply the Goldfeldquandt test\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIjiQ1NghmWN"
   },
   "source": [
    "## Final Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Sqvs4TMKtn"
   },
   "outputs": [],
   "source": [
    "olsmodel_final = sm.'_______' ## Complete the code to fit the final model\n",
    "print(olsmodel_final.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KbUJPfhhmWN"
   },
   "outputs": [],
   "source": [
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmodel_final_train_perf = model_performance_regression('_______') ## Complete the code to check the performance on train data\n",
    "olsmodel_final_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-WL13KBhmWN"
   },
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmodel_final_test_perf = model_performance_regression('_______') ## Complete the code to check performance on test data\n",
    "olsmodel_final_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BkZh6eHluZK"
   },
   "source": [
    "##  Actionable Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj1rya_NXY-y"
   },
   "source": [
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9V-w49_ULvd"
   },
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "W_Ho8VGKDu3S",
    "v_-uuGqH-qTt",
    "xxhpZv9y-qTw",
    "aodMV5D3-qTy",
    "qlzqMR1K-qTz",
    "KQi5ygTC-qT1",
    "5TcqcxbK-qT3",
    "nUCorhch-qT4",
    "xNr4bWoM-qT5",
    "Ch_TjRfF-qT5",
    "DhPuzWO7hmV8",
    "uxiNWC9JhmV8",
    "Ga_huJrnhmWE",
    "sbDJ_Vl8hmWI",
    "9Jnj657ShmWI",
    "OPB6dxxmWq1s",
    "4jUrmT40Wx7M",
    "QChrTYFaKvTk",
    "1JsqBJEohmWK",
    "9dgIbf7ALlYm",
    "a9GxSQf-qH8e",
    "SB4GanGWSOBT",
    "ulUc-ceISeyu",
    "j2hb9VujhmWL",
    "ykvaeUZ-TkjD",
    "VOxf_qsQqH-N",
    "FTnuLaLBXvdR",
    "aIjiQ1NghmWN",
    "2BkZh6eHluZK"
   ],
   "name": "SLF_Project_LearnerNotebook_LowCode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
